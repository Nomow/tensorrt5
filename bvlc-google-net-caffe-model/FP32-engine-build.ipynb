{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "import uff\n",
    "import tensorrt as trt\n",
    "import ctypes\n",
    "from matplotlib import pyplot as plt\n",
    "from six.moves import cPickle\n",
    "import logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializes logger for debugging purposes\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logging.debug(\"test\")\n",
    "TRT_LOGGER = trt.Logger(trt.Logger.INFO)\n",
    "curr_parent_dir =  os.path.dirname(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## information about data needed to parse model and build engine\n",
    "class ModelData(object):\n",
    "    MODEL_FILE = curr_parent_dir + \"/models/bvlc_googlenet/weights.caffemodel\"\n",
    "    DEPLOY_FILE = curr_parent_dir + \"/models/bvlc_googlenet/deploy.prototxt\"\n",
    "    INPUT_SHAPE = (3, 224, 224) # image dimensions - nchw\n",
    "    OUTPUT_NAME = \"prob\" # output layer name\n",
    "    DTYPE = trt.float32 # type of optimization FP32, FP16.\n",
    "    BATCH_SIZE = 500; # maximum batch size of engine to use for inference\n",
    "    WORKSPACE_SIZE = 1 << 30 # The maximum GPU temporary memory which the ICudaEngine can use at execution time.\n",
    "    ENGINE_NAME = \"fp32.engine\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorrt.tensorrt.IBlobNameToTensor object at 0x7fea51e479d0>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# parses model to trt\n",
    "with trt.Builder(TRT_LOGGER) as builder, builder.create_network() as network, trt.CaffeParser() as parser:\n",
    "    # builder argums for optimization\n",
    "    builder.max_batch_size = ModelData.BATCH_SIZE\n",
    "    builder.max_workspace_size = ModelData.WORKSPACE_SIZE  \n",
    "    # Load the Caffe model and parse it in order to populate the TensorRT network.\n",
    "    # This function returns an object that we can query to find tensors by name.\n",
    "    model_tensors = parser.parse(deploy=ModelData.DEPLOY_FILE, model=ModelData.MODEL_FILE, network=network, dtype=ModelData.DTYPE)\n",
    "    # For Caffe, we need to manually mark the output of the network.\n",
    "    # Since we know the name of the output tensor, we can find it in model_tensors.\n",
    "    print(model_tensors)\n",
    "    network.mark_output(model_tensors.find(ModelData.OUTPUT_NAME))\n",
    "\n",
    "    # builds engine\n",
    "    with builder.build_cuda_engine(network) as engine:\n",
    "        with open(ModelData.ENGINE_NAME, \"wb\") as f:\n",
    "            f.write(engine.serialize())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
